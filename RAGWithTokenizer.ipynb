{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48535d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To run transformers we would need GPU. Hence, we can you `https://colab.research.google.com/` with free T4 Runtime and execute below code.\n",
    "\n",
    "#Log-in to HuggingFace.co and generate write TOKEN and put it in colab secret\n",
    "\n",
    "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "!pip install -q requests==2.32.4 diffusers transformers==4.48.3 gradio langchain langchain-community faiss-cpu langchain_experimental bitsandbytes==0.46.0 accelerate==1.3.0\n",
    "\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,  BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "from google.colab import userdata\n",
    "import gradio as gr\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "\n",
    "\n",
    "hf_token=userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)\n",
    "\n",
    "\n",
    "loader = CSVLoader(file_path=\"/content/sample_data/california_housing_test.csv\", content_columns= ['longitude',\t'latitude',\t'housing_median_age',\t'total_rooms',\t'total_bedrooms',\t'population',\t'households',\t'median_income',\t'median_house_value'],\n",
    "                   metadata_columns=['longitude',\t'latitude',\t'housing_median_age',\t'total_rooms',\t'total_bedrooms',\t'population',\t'households',\t'median_income',\t'median_house_value'])\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = SemanticChunker(HuggingFaceEmbeddings)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings()\n",
    "\n",
    "vector = FAISS.from_documents(documents = documents, embedding=embedding)\n",
    "\n",
    "retriever = vector.as_retriever(search_type = \"similarity\", search_kwargs={\"k\":3})\n",
    "\n",
    "\n",
    "\n",
    "code_qwen = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
    "]\n",
    "\n",
    "quant_config= BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(code_qwen)   #initialize tokenizer\n",
    "tokenizer.pad_token=tokenizer.eos_token    #add padding\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(code_qwen, device_map=\"auto\", quantization_config=quant_config) #initialize llm model\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm, retriever = retriever)\n",
    "\n",
    "query = \"Please explain california housing test in couple of sentences\"\n",
    "\n",
    "print(chain.run(query))\n",
    "\n",
    "\n",
    "\n",
    "# memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# conversation_chain = ConversationalRetrievalChain.from_llm(llm=model, retriever=retriever, memory=memory)\n",
    "\n",
    "\n",
    "\n",
    "# result = conversation_chain.invoke({\"question\": query})\n",
    "\n",
    "# print(result[\"answer\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
